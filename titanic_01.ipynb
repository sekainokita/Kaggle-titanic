{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 필요한 라이브러리 임포트"
      ],
      "metadata": {
        "id": "tBFMQYkHGfHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "gcJe9f2Jh1CZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature encode(더미 생성후 원-핫 인코딩 방식)"
      ],
      "metadata": {
        "id": "3_K_yE8iGkt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# 결측치 처리\n",
        "train['Age'] = train['Age'].fillna(train['Age'].median()) # mean, median\n",
        "train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])\n",
        "train['Fare'] = train['Fare'].fillna(train['Fare'].mean())\n",
        "\n",
        "# 가족 크기 계산\n",
        "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
        "\n",
        "# 이름에서 직함 추출\n",
        "train['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "# 직함 그룹화\n",
        "Other_titles = ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir.', 'Jonkheer', 'Dona']\n",
        "train.loc[train['Title'].isin(Other_titles), 'Title'] = 'Other'\n",
        "train['Title'] = train['Title'].replace('Mlle', 'Miss')\n",
        "train['Title'] = train['Title'].replace('Ms', 'Miss')\n",
        "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# 직함 매핑\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\n",
        "train['Title'] = train['Title'].map(title_mapping)\n",
        "train['Title'] = train['Title'].fillna(0)\n",
        "\n",
        "# 객실 정보 처리\n",
        "train['HasCabin'] = train['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
        "\n",
        "# 범주형 변수 처리\n",
        "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
        "train['Embarked'] = train['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 원-핫 인코딩을 위한 더미 변수 생성\n",
        "embarked_dummies = pd.get_dummies(train['Embarked'], prefix='Emb')\n",
        "pclass_dummies = pd.get_dummies(train['Pclass'], prefix='Pclass')\n",
        "\n",
        "# 최종 특성 선택\n",
        "features = ['Sex', 'Age', 'Fare', 'FamilySize', 'Title', 'HasCabin', 'SibSp', 'Parch']\n",
        "\n",
        "# 제외한 특성 -\n",
        "#-------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# 최종 데이터프레임 생성\n",
        "X = pd.concat([\n",
        "    train[features],\n",
        "    embarked_dummies,\n",
        "    pclass_dummies\n",
        "], axis=1)\n",
        "\n",
        "# 타겟 변수\n",
        "y = train['Survived']\n",
        "\n",
        "print(\"전처리된 특성 개수:\", X.shape)\n",
        "print(\"전처리된 특성 목록:\", list(X.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TujYaehn9ML",
        "outputId": "0a9901d0-ae68-494b-de78-170a850d6e39"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리된 특성 개수: (891, 14)\n",
            "전처리된 특성 목록: ['Sex', 'Age', 'Fare', 'FamilySize', 'Title', 'HasCabin', 'SibSp', 'Parch', 'Emb_0', 'Emb_1', 'Emb_2', 'Pclass_1', 'Pclass_2', 'Pclass_3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# feature encode(라벨 인코딩 방식)"
      ],
      "metadata": {
        "id": "dy0xz0doLBxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# 결측치 처리\n",
        "train['Age'] = train['Age'].fillna(train['Age'].mean())\n",
        "train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])\n",
        "train['Fare'] = train['Fare'].fillna(train['Fare'].mean())\n",
        "\n",
        "# 가족 크기 계산\n",
        "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
        "\n",
        "# 이름에서 직함 추출\n",
        "train['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "# 직함 그룹화\n",
        "Other_titles = ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir.', 'Jonkheer', 'Dona']\n",
        "train.loc[train['Title'].isin(Other_titles), 'Title'] = 'Other'\n",
        "train['Title'] = train['Title'].replace('Mlle', 'Miss')\n",
        "train['Title'] = train['Title'].replace('Ms', 'Miss')\n",
        "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# 직함 매핑\n",
        "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\n",
        "train['Title'] = train['Title'].map(title_mapping)\n",
        "train['Title'] = train['Title'].fillna(0)\n",
        "\n",
        "# 객실 정보 처리\n",
        "train['HasCabin'] = train['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
        "\n",
        "# Label 인코딩 사용\n",
        "label_encoder = LabelEncoder()\n",
        "train['Embarked_encoded'] = label_encoder.fit_transform(train['Embarked'])\n",
        "train['Pclass_encoded'] = label_encoder.fit_transform(train['Pclass'].astype(str))\n",
        "\n",
        "# 최종 특성 선택\n",
        "features = ['Sex', 'Age', 'Fare', 'FamilySize', 'Title', 'HasCabin',\n",
        "           'SibSp', 'Parch', 'Embarked_encoded', 'Pclass_encoded']\n",
        "\n",
        "# 최종 데이터프레임 생성\n",
        "X = pd.concat([\n",
        "    train[features]\n",
        "], axis=1)\n",
        "\n",
        "# 타겟 변수\n",
        "y = train['Survived']\n",
        "\n",
        "print(\"전처리된 특성 개수:\", X.shape)\n",
        "print(\"전처리된 특성 목록:\", list(X.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4QbUqSjEGxs",
        "outputId": "5e79bbd8-fa45-42bd-9365-255e62fe7437"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전처리된 특성 개수: (891, 10)\n",
            "전처리된 특성 목록: ['Sex', 'Age', 'Fare', 'FamilySize', 'Title', 'HasCabin', 'SibSp', 'Parch', 'Embarked_encoded', 'Pclass_encoded']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 스케일링 및 훈련 모델 학습 과정"
      ],
      "metadata": {
        "id": "341fywMpGZjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 스케일링\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.FloatTensor(y_train.values)\n",
        "X_val = torch.FloatTensor(X_val)\n",
        "y_val = torch.FloatTensor(y_val.values)\n",
        "\n",
        "# 모델 정의\n",
        "class TitanicNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(TitanicNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# 모델 초기화\n",
        "model = TitanicNet(X_train.shape[1])\n",
        "\n",
        "# 손실 함수와 옵티마이저 정의\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습\n",
        "num_epochs = 5000\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        batch_X = X_train[i:i+batch_size]\n",
        "        batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y.unsqueeze(1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 검증\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_outputs = model(X_val)\n",
        "    val_predictions = (val_outputs > 0.5).float()\n",
        "    accuracy = (val_predictions == y_val.unsqueeze(1)).float().mean()\n",
        "    print(f'Validation Accuracy: {accuracy.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKefWhr5oCtG",
        "outputId": "acfe3d01-7a07-4e50-82bd-fb6de07264e1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/5000], Loss: 0.1851\n",
            "Epoch [20/5000], Loss: 0.1732\n",
            "Epoch [30/5000], Loss: 0.1714\n",
            "Epoch [40/5000], Loss: 0.1706\n",
            "Epoch [50/5000], Loss: 0.1678\n",
            "Epoch [60/5000], Loss: 0.1685\n",
            "Epoch [70/5000], Loss: 0.1682\n",
            "Epoch [80/5000], Loss: 0.1683\n",
            "Epoch [90/5000], Loss: 0.1672\n",
            "Epoch [100/5000], Loss: 0.1611\n",
            "Epoch [110/5000], Loss: 0.1559\n",
            "Epoch [120/5000], Loss: 0.1484\n",
            "Epoch [130/5000], Loss: 0.1418\n",
            "Epoch [140/5000], Loss: 0.1371\n",
            "Epoch [150/5000], Loss: 0.1304\n",
            "Epoch [160/5000], Loss: 0.1246\n",
            "Epoch [170/5000], Loss: 0.1205\n",
            "Epoch [180/5000], Loss: 0.1171\n",
            "Epoch [190/5000], Loss: 0.1158\n",
            "Epoch [200/5000], Loss: 0.1138\n",
            "Epoch [210/5000], Loss: 0.1111\n",
            "Epoch [220/5000], Loss: 0.1088\n",
            "Epoch [230/5000], Loss: 0.1052\n",
            "Epoch [240/5000], Loss: 0.1041\n",
            "Epoch [250/5000], Loss: 0.1041\n",
            "Epoch [260/5000], Loss: 0.1017\n",
            "Epoch [270/5000], Loss: 0.1002\n",
            "Epoch [280/5000], Loss: 0.0991\n",
            "Epoch [290/5000], Loss: 0.0994\n",
            "Epoch [300/5000], Loss: 0.0979\n",
            "Epoch [310/5000], Loss: 0.0997\n",
            "Epoch [320/5000], Loss: 0.0983\n",
            "Epoch [330/5000], Loss: 0.0972\n",
            "Epoch [340/5000], Loss: 0.0974\n",
            "Epoch [350/5000], Loss: 0.0979\n",
            "Epoch [360/5000], Loss: 0.0964\n",
            "Epoch [370/5000], Loss: 0.0974\n",
            "Epoch [380/5000], Loss: 0.0957\n",
            "Epoch [390/5000], Loss: 0.0967\n",
            "Epoch [400/5000], Loss: 0.0962\n",
            "Epoch [410/5000], Loss: 0.0965\n",
            "Epoch [420/5000], Loss: 0.0962\n",
            "Epoch [430/5000], Loss: 0.0954\n",
            "Epoch [440/5000], Loss: 0.0972\n",
            "Epoch [450/5000], Loss: 0.0972\n",
            "Epoch [460/5000], Loss: 0.0972\n",
            "Epoch [470/5000], Loss: 0.0969\n",
            "Epoch [480/5000], Loss: 0.0966\n",
            "Epoch [490/5000], Loss: 0.0953\n",
            "Epoch [500/5000], Loss: 0.0955\n",
            "Epoch [510/5000], Loss: 0.0953\n",
            "Epoch [520/5000], Loss: 0.0941\n",
            "Epoch [530/5000], Loss: 0.0935\n",
            "Epoch [540/5000], Loss: 0.0942\n",
            "Epoch [550/5000], Loss: 0.0951\n",
            "Epoch [560/5000], Loss: 0.0927\n",
            "Epoch [570/5000], Loss: 0.0935\n",
            "Epoch [580/5000], Loss: 0.0934\n",
            "Epoch [590/5000], Loss: 0.0943\n",
            "Epoch [600/5000], Loss: 0.0947\n",
            "Epoch [610/5000], Loss: 0.0957\n",
            "Epoch [620/5000], Loss: 0.0943\n",
            "Epoch [630/5000], Loss: 0.0929\n",
            "Epoch [640/5000], Loss: 0.0940\n",
            "Epoch [650/5000], Loss: 0.0933\n",
            "Epoch [660/5000], Loss: 0.0940\n",
            "Epoch [670/5000], Loss: 0.0937\n",
            "Epoch [680/5000], Loss: 0.0935\n",
            "Epoch [690/5000], Loss: 0.0940\n",
            "Epoch [700/5000], Loss: 0.0938\n",
            "Epoch [710/5000], Loss: 0.0939\n",
            "Epoch [720/5000], Loss: 0.0956\n",
            "Epoch [730/5000], Loss: 0.0933\n",
            "Epoch [740/5000], Loss: 0.0945\n",
            "Epoch [750/5000], Loss: 0.0894\n",
            "Epoch [760/5000], Loss: 0.0934\n",
            "Epoch [770/5000], Loss: 0.0931\n",
            "Epoch [780/5000], Loss: 0.0919\n",
            "Epoch [790/5000], Loss: 0.0950\n",
            "Epoch [800/5000], Loss: 0.0938\n",
            "Epoch [810/5000], Loss: 0.0947\n",
            "Epoch [820/5000], Loss: 0.0966\n",
            "Epoch [830/5000], Loss: 0.0913\n",
            "Epoch [840/5000], Loss: 0.0921\n",
            "Epoch [850/5000], Loss: 0.0947\n",
            "Epoch [860/5000], Loss: 0.0931\n",
            "Epoch [870/5000], Loss: 0.0942\n",
            "Epoch [880/5000], Loss: 0.0934\n",
            "Epoch [890/5000], Loss: 0.0931\n",
            "Epoch [900/5000], Loss: 0.0922\n",
            "Epoch [910/5000], Loss: 0.0907\n",
            "Epoch [920/5000], Loss: 0.0926\n",
            "Epoch [930/5000], Loss: 0.0905\n",
            "Epoch [940/5000], Loss: 0.0939\n",
            "Epoch [950/5000], Loss: 0.0946\n",
            "Epoch [960/5000], Loss: 0.0957\n",
            "Epoch [970/5000], Loss: 0.0915\n",
            "Epoch [980/5000], Loss: 0.0931\n",
            "Epoch [990/5000], Loss: 0.0888\n",
            "Epoch [1000/5000], Loss: 0.0896\n",
            "Epoch [1010/5000], Loss: 0.0919\n",
            "Epoch [1020/5000], Loss: 0.0907\n",
            "Epoch [1030/5000], Loss: 0.0907\n",
            "Epoch [1040/5000], Loss: 0.0904\n",
            "Epoch [1050/5000], Loss: 0.0922\n",
            "Epoch [1060/5000], Loss: 0.0905\n",
            "Epoch [1070/5000], Loss: 0.0922\n",
            "Epoch [1080/5000], Loss: 0.0892\n",
            "Epoch [1090/5000], Loss: 0.0899\n",
            "Epoch [1100/5000], Loss: 0.0877\n",
            "Epoch [1110/5000], Loss: 0.0867\n",
            "Epoch [1120/5000], Loss: 0.0895\n",
            "Epoch [1130/5000], Loss: 0.0888\n",
            "Epoch [1140/5000], Loss: 0.0893\n",
            "Epoch [1150/5000], Loss: 0.0901\n",
            "Epoch [1160/5000], Loss: 0.0884\n",
            "Epoch [1170/5000], Loss: 0.0874\n",
            "Epoch [1180/5000], Loss: 0.0892\n",
            "Epoch [1190/5000], Loss: 0.0877\n",
            "Epoch [1200/5000], Loss: 0.0871\n",
            "Epoch [1210/5000], Loss: 0.0859\n",
            "Epoch [1220/5000], Loss: 0.0890\n",
            "Epoch [1230/5000], Loss: 0.0890\n",
            "Epoch [1240/5000], Loss: 0.0880\n",
            "Epoch [1250/5000], Loss: 0.0882\n",
            "Epoch [1260/5000], Loss: 0.0837\n",
            "Epoch [1270/5000], Loss: 0.0848\n",
            "Epoch [1280/5000], Loss: 0.0834\n",
            "Epoch [1290/5000], Loss: 0.0887\n",
            "Epoch [1300/5000], Loss: 0.0886\n",
            "Epoch [1310/5000], Loss: 0.0824\n",
            "Epoch [1320/5000], Loss: 0.0854\n",
            "Epoch [1330/5000], Loss: 0.0864\n",
            "Epoch [1340/5000], Loss: 0.0826\n",
            "Epoch [1350/5000], Loss: 0.0824\n",
            "Epoch [1360/5000], Loss: 0.0852\n",
            "Epoch [1370/5000], Loss: 0.0825\n",
            "Epoch [1380/5000], Loss: 0.0855\n",
            "Epoch [1390/5000], Loss: 0.0815\n",
            "Epoch [1400/5000], Loss: 0.0798\n",
            "Epoch [1410/5000], Loss: 0.0841\n",
            "Epoch [1420/5000], Loss: 0.0821\n",
            "Epoch [1430/5000], Loss: 0.0857\n",
            "Epoch [1440/5000], Loss: 0.0844\n",
            "Epoch [1450/5000], Loss: 0.0828\n",
            "Epoch [1460/5000], Loss: 0.0802\n",
            "Epoch [1470/5000], Loss: 0.0802\n",
            "Epoch [1480/5000], Loss: 0.0808\n",
            "Epoch [1490/5000], Loss: 0.0795\n",
            "Epoch [1500/5000], Loss: 0.0841\n",
            "Epoch [1510/5000], Loss: 0.0829\n",
            "Epoch [1520/5000], Loss: 0.0834\n",
            "Epoch [1530/5000], Loss: 0.0825\n",
            "Epoch [1540/5000], Loss: 0.0840\n",
            "Epoch [1550/5000], Loss: 0.0821\n",
            "Epoch [1560/5000], Loss: 0.0818\n",
            "Epoch [1570/5000], Loss: 0.0794\n",
            "Epoch [1580/5000], Loss: 0.0838\n",
            "Epoch [1590/5000], Loss: 0.0787\n",
            "Epoch [1600/5000], Loss: 0.0804\n",
            "Epoch [1610/5000], Loss: 0.0834\n",
            "Epoch [1620/5000], Loss: 0.0784\n",
            "Epoch [1630/5000], Loss: 0.0809\n",
            "Epoch [1640/5000], Loss: 0.0764\n",
            "Epoch [1650/5000], Loss: 0.0783\n",
            "Epoch [1660/5000], Loss: 0.0780\n",
            "Epoch [1670/5000], Loss: 0.0785\n",
            "Epoch [1680/5000], Loss: 0.0786\n",
            "Epoch [1690/5000], Loss: 0.0784\n",
            "Epoch [1700/5000], Loss: 0.0708\n",
            "Epoch [1710/5000], Loss: 0.0775\n",
            "Epoch [1720/5000], Loss: 0.0759\n",
            "Epoch [1730/5000], Loss: 0.0773\n",
            "Epoch [1740/5000], Loss: 0.0748\n",
            "Epoch [1750/5000], Loss: 0.0730\n",
            "Epoch [1760/5000], Loss: 0.0784\n",
            "Epoch [1770/5000], Loss: 0.0760\n",
            "Epoch [1780/5000], Loss: 0.0766\n",
            "Epoch [1790/5000], Loss: 0.0735\n",
            "Epoch [1800/5000], Loss: 0.0783\n",
            "Epoch [1810/5000], Loss: 0.0774\n",
            "Epoch [1820/5000], Loss: 0.0752\n",
            "Epoch [1830/5000], Loss: 0.0752\n",
            "Epoch [1840/5000], Loss: 0.0701\n",
            "Epoch [1850/5000], Loss: 0.0734\n",
            "Epoch [1860/5000], Loss: 0.0735\n",
            "Epoch [1870/5000], Loss: 0.0738\n",
            "Epoch [1880/5000], Loss: 0.0721\n",
            "Epoch [1890/5000], Loss: 0.0746\n",
            "Epoch [1900/5000], Loss: 0.0738\n",
            "Epoch [1910/5000], Loss: 0.0732\n",
            "Epoch [1920/5000], Loss: 0.0708\n",
            "Epoch [1930/5000], Loss: 0.0682\n",
            "Epoch [1940/5000], Loss: 0.0720\n",
            "Epoch [1950/5000], Loss: 0.0718\n",
            "Epoch [1960/5000], Loss: 0.0711\n",
            "Epoch [1970/5000], Loss: 0.0719\n",
            "Epoch [1980/5000], Loss: 0.0687\n",
            "Epoch [1990/5000], Loss: 0.0703\n",
            "Epoch [2000/5000], Loss: 0.0732\n",
            "Epoch [2010/5000], Loss: 0.0703\n",
            "Epoch [2020/5000], Loss: 0.0713\n",
            "Epoch [2030/5000], Loss: 0.0722\n",
            "Epoch [2040/5000], Loss: 0.0706\n",
            "Epoch [2050/5000], Loss: 0.0735\n",
            "Epoch [2060/5000], Loss: 0.0703\n",
            "Epoch [2070/5000], Loss: 0.0692\n",
            "Epoch [2080/5000], Loss: 0.0697\n",
            "Epoch [2090/5000], Loss: 0.0704\n",
            "Epoch [2100/5000], Loss: 0.0675\n",
            "Epoch [2110/5000], Loss: 0.0650\n",
            "Epoch [2120/5000], Loss: 0.0668\n",
            "Epoch [2130/5000], Loss: 0.0688\n",
            "Epoch [2140/5000], Loss: 0.0660\n",
            "Epoch [2150/5000], Loss: 0.0676\n",
            "Epoch [2160/5000], Loss: 0.0687\n",
            "Epoch [2170/5000], Loss: 0.0673\n",
            "Epoch [2180/5000], Loss: 0.0678\n",
            "Epoch [2190/5000], Loss: 0.0661\n",
            "Epoch [2200/5000], Loss: 0.0671\n",
            "Epoch [2210/5000], Loss: 0.0658\n",
            "Epoch [2220/5000], Loss: 0.0642\n",
            "Epoch [2230/5000], Loss: 0.0662\n",
            "Epoch [2240/5000], Loss: 0.0615\n",
            "Epoch [2250/5000], Loss: 0.0638\n",
            "Epoch [2260/5000], Loss: 0.0672\n",
            "Epoch [2270/5000], Loss: 0.0617\n",
            "Epoch [2280/5000], Loss: 0.0655\n",
            "Epoch [2290/5000], Loss: 0.0624\n",
            "Epoch [2300/5000], Loss: 0.0662\n",
            "Epoch [2310/5000], Loss: 0.0641\n",
            "Epoch [2320/5000], Loss: 0.0638\n",
            "Epoch [2330/5000], Loss: 0.0626\n",
            "Epoch [2340/5000], Loss: 0.0653\n",
            "Epoch [2350/5000], Loss: 0.0625\n",
            "Epoch [2360/5000], Loss: 0.0581\n",
            "Epoch [2370/5000], Loss: 0.0620\n",
            "Epoch [2380/5000], Loss: 0.0613\n",
            "Epoch [2390/5000], Loss: 0.0644\n",
            "Epoch [2400/5000], Loss: 0.0598\n",
            "Epoch [2410/5000], Loss: 0.0630\n",
            "Epoch [2420/5000], Loss: 0.0636\n",
            "Epoch [2430/5000], Loss: 0.0626\n",
            "Epoch [2440/5000], Loss: 0.0615\n",
            "Epoch [2450/5000], Loss: 0.0632\n",
            "Epoch [2460/5000], Loss: 0.0613\n",
            "Epoch [2470/5000], Loss: 0.0623\n",
            "Epoch [2480/5000], Loss: 0.0629\n",
            "Epoch [2490/5000], Loss: 0.0616\n",
            "Epoch [2500/5000], Loss: 0.0600\n",
            "Epoch [2510/5000], Loss: 0.0617\n",
            "Epoch [2520/5000], Loss: 0.0598\n",
            "Epoch [2530/5000], Loss: 0.0600\n",
            "Epoch [2540/5000], Loss: 0.0640\n",
            "Epoch [2550/5000], Loss: 0.0600\n",
            "Epoch [2560/5000], Loss: 0.0588\n",
            "Epoch [2570/5000], Loss: 0.0576\n",
            "Epoch [2580/5000], Loss: 0.0582\n",
            "Epoch [2590/5000], Loss: 0.0558\n",
            "Epoch [2600/5000], Loss: 0.0552\n",
            "Epoch [2610/5000], Loss: 0.0590\n",
            "Epoch [2620/5000], Loss: 0.0581\n",
            "Epoch [2630/5000], Loss: 0.0575\n",
            "Epoch [2640/5000], Loss: 0.0585\n",
            "Epoch [2650/5000], Loss: 0.0548\n",
            "Epoch [2660/5000], Loss: 0.0544\n",
            "Epoch [2670/5000], Loss: 0.0551\n",
            "Epoch [2680/5000], Loss: 0.0546\n",
            "Epoch [2690/5000], Loss: 0.0558\n",
            "Epoch [2700/5000], Loss: 0.0564\n",
            "Epoch [2710/5000], Loss: 0.0581\n",
            "Epoch [2720/5000], Loss: 0.0506\n",
            "Epoch [2730/5000], Loss: 0.0580\n",
            "Epoch [2740/5000], Loss: 0.0560\n",
            "Epoch [2750/5000], Loss: 0.0567\n",
            "Epoch [2760/5000], Loss: 0.0539\n",
            "Epoch [2770/5000], Loss: 0.0510\n",
            "Epoch [2780/5000], Loss: 0.0523\n",
            "Epoch [2790/5000], Loss: 0.0508\n",
            "Epoch [2800/5000], Loss: 0.0491\n",
            "Epoch [2810/5000], Loss: 0.0507\n",
            "Epoch [2820/5000], Loss: 0.0500\n",
            "Epoch [2830/5000], Loss: 0.0550\n",
            "Epoch [2840/5000], Loss: 0.0521\n",
            "Epoch [2850/5000], Loss: 0.0526\n",
            "Epoch [2860/5000], Loss: 0.0515\n",
            "Epoch [2870/5000], Loss: 0.0561\n",
            "Epoch [2880/5000], Loss: 0.0543\n",
            "Epoch [2890/5000], Loss: 0.0464\n",
            "Epoch [2900/5000], Loss: 0.0478\n",
            "Epoch [2910/5000], Loss: 0.0444\n",
            "Epoch [2920/5000], Loss: 0.0475\n",
            "Epoch [2930/5000], Loss: 0.0505\n",
            "Epoch [2940/5000], Loss: 0.0485\n",
            "Epoch [2950/5000], Loss: 0.0534\n",
            "Epoch [2960/5000], Loss: 0.0508\n",
            "Epoch [2970/5000], Loss: 0.0525\n",
            "Epoch [2980/5000], Loss: 0.0513\n",
            "Epoch [2990/5000], Loss: 0.0477\n",
            "Epoch [3000/5000], Loss: 0.0454\n",
            "Epoch [3010/5000], Loss: 0.0483\n",
            "Epoch [3020/5000], Loss: 0.0446\n",
            "Epoch [3030/5000], Loss: 0.0431\n",
            "Epoch [3040/5000], Loss: 0.0255\n",
            "Epoch [3050/5000], Loss: 0.0376\n",
            "Epoch [3060/5000], Loss: 0.0385\n",
            "Epoch [3070/5000], Loss: 0.0393\n",
            "Epoch [3080/5000], Loss: 0.0433\n",
            "Epoch [3090/5000], Loss: 0.0456\n",
            "Epoch [3100/5000], Loss: 0.0481\n",
            "Epoch [3110/5000], Loss: 0.0466\n",
            "Epoch [3120/5000], Loss: 0.0500\n",
            "Epoch [3130/5000], Loss: 0.0494\n",
            "Epoch [3140/5000], Loss: 0.0497\n",
            "Epoch [3150/5000], Loss: 0.0497\n",
            "Epoch [3160/5000], Loss: 0.0469\n",
            "Epoch [3170/5000], Loss: 0.0497\n",
            "Epoch [3180/5000], Loss: 0.0475\n",
            "Epoch [3190/5000], Loss: 0.0461\n",
            "Epoch [3200/5000], Loss: 0.0458\n",
            "Epoch [3210/5000], Loss: 0.0460\n",
            "Epoch [3220/5000], Loss: 0.0412\n",
            "Epoch [3230/5000], Loss: 0.0365\n",
            "Epoch [3240/5000], Loss: 0.0367\n",
            "Epoch [3250/5000], Loss: 0.0363\n",
            "Epoch [3260/5000], Loss: 0.0405\n",
            "Epoch [3270/5000], Loss: 0.0402\n",
            "Epoch [3280/5000], Loss: 0.0370\n",
            "Epoch [3290/5000], Loss: 0.0417\n",
            "Epoch [3300/5000], Loss: 0.0412\n",
            "Epoch [3310/5000], Loss: 0.0421\n",
            "Epoch [3320/5000], Loss: 0.0462\n",
            "Epoch [3330/5000], Loss: 0.0490\n",
            "Epoch [3340/5000], Loss: 0.0433\n",
            "Epoch [3350/5000], Loss: 0.0457\n",
            "Epoch [3360/5000], Loss: 0.0450\n",
            "Epoch [3370/5000], Loss: 0.0478\n",
            "Epoch [3380/5000], Loss: 0.0423\n",
            "Epoch [3390/5000], Loss: 0.0425\n",
            "Epoch [3400/5000], Loss: 0.0417\n",
            "Epoch [3410/5000], Loss: 0.0399\n",
            "Epoch [3420/5000], Loss: 0.0306\n",
            "Epoch [3430/5000], Loss: 0.0352\n",
            "Epoch [3440/5000], Loss: 0.0331\n",
            "Epoch [3450/5000], Loss: 0.0334\n",
            "Epoch [3460/5000], Loss: 0.0336\n",
            "Epoch [3470/5000], Loss: 0.0349\n",
            "Epoch [3480/5000], Loss: 0.0363\n",
            "Epoch [3490/5000], Loss: 0.0351\n",
            "Epoch [3500/5000], Loss: 0.0357\n",
            "Epoch [3510/5000], Loss: 0.0353\n",
            "Epoch [3520/5000], Loss: 0.0329\n",
            "Epoch [3530/5000], Loss: 0.0354\n",
            "Epoch [3540/5000], Loss: 0.0383\n",
            "Epoch [3550/5000], Loss: 0.0365\n",
            "Epoch [3560/5000], Loss: 0.0390\n",
            "Epoch [3570/5000], Loss: 0.0414\n",
            "Epoch [3580/5000], Loss: 0.0390\n",
            "Epoch [3590/5000], Loss: 0.0446\n",
            "Epoch [3600/5000], Loss: 0.0336\n",
            "Epoch [3610/5000], Loss: 0.0549\n",
            "Epoch [3620/5000], Loss: 0.0282\n",
            "Epoch [3630/5000], Loss: 0.0279\n",
            "Epoch [3640/5000], Loss: 0.0283\n",
            "Epoch [3650/5000], Loss: 0.0301\n",
            "Epoch [3660/5000], Loss: 0.0292\n",
            "Epoch [3670/5000], Loss: 0.0271\n",
            "Epoch [3680/5000], Loss: 0.0262\n",
            "Epoch [3690/5000], Loss: 0.0285\n",
            "Epoch [3700/5000], Loss: 0.0270\n",
            "Epoch [3710/5000], Loss: 0.0305\n",
            "Epoch [3720/5000], Loss: 0.0285\n",
            "Epoch [3730/5000], Loss: 0.0294\n",
            "Epoch [3740/5000], Loss: 0.0271\n",
            "Epoch [3750/5000], Loss: 0.0278\n",
            "Epoch [3760/5000], Loss: 0.0288\n",
            "Epoch [3770/5000], Loss: 0.0296\n",
            "Epoch [3780/5000], Loss: 0.0284\n",
            "Epoch [3790/5000], Loss: 0.0275\n",
            "Epoch [3800/5000], Loss: 0.0268\n",
            "Epoch [3810/5000], Loss: 0.0279\n",
            "Epoch [3820/5000], Loss: 0.0288\n",
            "Epoch [3830/5000], Loss: 0.0281\n",
            "Epoch [3840/5000], Loss: 0.0265\n",
            "Epoch [3850/5000], Loss: 0.0289\n",
            "Epoch [3860/5000], Loss: 0.0274\n",
            "Epoch [3870/5000], Loss: 0.0279\n",
            "Epoch [3880/5000], Loss: 0.0273\n",
            "Epoch [3890/5000], Loss: 0.0260\n",
            "Epoch [3900/5000], Loss: 0.0268\n",
            "Epoch [3910/5000], Loss: 0.0296\n",
            "Epoch [3920/5000], Loss: 0.0238\n",
            "Epoch [3930/5000], Loss: 0.0282\n",
            "Epoch [3940/5000], Loss: 0.0286\n",
            "Epoch [3950/5000], Loss: 0.0275\n",
            "Epoch [3960/5000], Loss: 0.0282\n",
            "Epoch [3970/5000], Loss: 0.0277\n",
            "Epoch [3980/5000], Loss: 0.0268\n",
            "Epoch [3990/5000], Loss: 0.0258\n",
            "Epoch [4000/5000], Loss: 0.0253\n",
            "Epoch [4010/5000], Loss: 0.0279\n",
            "Epoch [4020/5000], Loss: 0.0256\n",
            "Epoch [4030/5000], Loss: 0.0262\n",
            "Epoch [4040/5000], Loss: 0.0282\n",
            "Epoch [4050/5000], Loss: 0.0245\n",
            "Epoch [4060/5000], Loss: 0.0244\n",
            "Epoch [4070/5000], Loss: 0.0260\n",
            "Epoch [4080/5000], Loss: 0.0309\n",
            "Epoch [4090/5000], Loss: 0.0305\n",
            "Epoch [4100/5000], Loss: 0.0287\n",
            "Epoch [4110/5000], Loss: 0.0271\n",
            "Epoch [4120/5000], Loss: 0.0271\n",
            "Epoch [4130/5000], Loss: 0.0288\n",
            "Epoch [4140/5000], Loss: 0.0316\n",
            "Epoch [4150/5000], Loss: 0.0275\n",
            "Epoch [4160/5000], Loss: 0.0283\n",
            "Epoch [4170/5000], Loss: 0.0293\n",
            "Epoch [4180/5000], Loss: 0.0278\n",
            "Epoch [4190/5000], Loss: 0.0289\n",
            "Epoch [4200/5000], Loss: 0.0293\n",
            "Epoch [4210/5000], Loss: 0.0290\n",
            "Epoch [4220/5000], Loss: 0.0230\n",
            "Epoch [4230/5000], Loss: 0.0271\n",
            "Epoch [4240/5000], Loss: 0.0252\n",
            "Epoch [4250/5000], Loss: 0.0260\n",
            "Epoch [4260/5000], Loss: 0.0274\n",
            "Epoch [4270/5000], Loss: 0.0301\n",
            "Epoch [4280/5000], Loss: 0.0271\n",
            "Epoch [4290/5000], Loss: 0.0257\n",
            "Epoch [4300/5000], Loss: 0.0282\n",
            "Epoch [4310/5000], Loss: 0.0283\n",
            "Epoch [4320/5000], Loss: 0.0249\n",
            "Epoch [4330/5000], Loss: 0.0305\n",
            "Epoch [4340/5000], Loss: 0.0277\n",
            "Epoch [4350/5000], Loss: 0.0271\n",
            "Epoch [4360/5000], Loss: 0.0285\n",
            "Epoch [4370/5000], Loss: 0.0293\n",
            "Epoch [4380/5000], Loss: 0.0297\n",
            "Epoch [4390/5000], Loss: 0.0370\n",
            "Epoch [4400/5000], Loss: 0.0283\n",
            "Epoch [4410/5000], Loss: 0.0309\n",
            "Epoch [4420/5000], Loss: 0.0284\n",
            "Epoch [4430/5000], Loss: 0.0258\n",
            "Epoch [4440/5000], Loss: 0.0265\n",
            "Epoch [4450/5000], Loss: 0.0276\n",
            "Epoch [4460/5000], Loss: 0.0357\n",
            "Epoch [4470/5000], Loss: 0.0368\n",
            "Epoch [4480/5000], Loss: 0.0274\n",
            "Epoch [4490/5000], Loss: 0.0294\n",
            "Epoch [4500/5000], Loss: 0.0295\n",
            "Epoch [4510/5000], Loss: 0.0287\n",
            "Epoch [4520/5000], Loss: 0.0261\n",
            "Epoch [4530/5000], Loss: 0.0329\n",
            "Epoch [4540/5000], Loss: 0.0325\n",
            "Epoch [4550/5000], Loss: 0.0258\n",
            "Epoch [4560/5000], Loss: 0.0257\n",
            "Epoch [4570/5000], Loss: 0.0287\n",
            "Epoch [4580/5000], Loss: 0.0345\n",
            "Epoch [4590/5000], Loss: 0.0368\n",
            "Epoch [4600/5000], Loss: 0.0329\n",
            "Epoch [4610/5000], Loss: 0.0331\n",
            "Epoch [4620/5000], Loss: 0.0366\n",
            "Epoch [4630/5000], Loss: 0.0386\n",
            "Epoch [4640/5000], Loss: 0.0198\n",
            "Epoch [4650/5000], Loss: 0.0206\n",
            "Epoch [4660/5000], Loss: 0.0251\n",
            "Epoch [4670/5000], Loss: 0.0254\n",
            "Epoch [4680/5000], Loss: 0.0240\n",
            "Epoch [4690/5000], Loss: 0.0263\n",
            "Epoch [4700/5000], Loss: 0.0250\n",
            "Epoch [4710/5000], Loss: 0.0263\n",
            "Epoch [4720/5000], Loss: 0.0182\n",
            "Epoch [4730/5000], Loss: 0.0197\n",
            "Epoch [4740/5000], Loss: 0.0247\n",
            "Epoch [4750/5000], Loss: 0.0300\n",
            "Epoch [4760/5000], Loss: 0.0257\n",
            "Epoch [4770/5000], Loss: 0.0220\n",
            "Epoch [4780/5000], Loss: 0.0173\n",
            "Epoch [4790/5000], Loss: 0.0181\n",
            "Epoch [4800/5000], Loss: 0.0241\n",
            "Epoch [4810/5000], Loss: 0.0258\n",
            "Epoch [4820/5000], Loss: 0.0276\n",
            "Epoch [4830/5000], Loss: 0.0261\n",
            "Epoch [4840/5000], Loss: 0.0271\n",
            "Epoch [4850/5000], Loss: 0.0338\n",
            "Epoch [4860/5000], Loss: 0.0310\n",
            "Epoch [4870/5000], Loss: 0.0173\n",
            "Epoch [4880/5000], Loss: 0.0184\n",
            "Epoch [4890/5000], Loss: 0.0263\n",
            "Epoch [4900/5000], Loss: 0.0267\n",
            "Epoch [4910/5000], Loss: 0.0249\n",
            "Epoch [4920/5000], Loss: 0.0249\n",
            "Epoch [4930/5000], Loss: 0.0153\n",
            "Epoch [4940/5000], Loss: 0.0193\n",
            "Epoch [4950/5000], Loss: 0.0213\n",
            "Epoch [4960/5000], Loss: 0.0290\n",
            "Epoch [4970/5000], Loss: 0.0276\n",
            "Epoch [4980/5000], Loss: 0.0259\n",
            "Epoch [4990/5000], Loss: 0.0274\n",
            "Epoch [5000/5000], Loss: 0.0265\n",
            "Validation Accuracy: 0.8045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 똑같이 test data 전처리 후 submission으로 저장"
      ],
      "metadata": {
        "id": "m6iCGUJkFSJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 전처리 (학습 데이터와 동일한 방식으로)\n",
        "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
        "test['Fare'] = test['Fare'].fillna(test['Fare'].mean())\n",
        "test['Embarked'] = test['Embarked'].fillna(test['Embarked'].mode()[0])\n",
        "\n",
        "# 가족 크기 계산\n",
        "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
        "\n",
        "# 이름에서 직함 추출\n",
        "test['Title'] = test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "# 직함 그룹화\n",
        "test.loc[test['Title'].isin(Other_titles), 'Title'] = 'Other'\n",
        "test['Title'] = test['Title'].replace('Mlle', 'Miss')\n",
        "test['Title'] = test['Title'].replace('Ms', 'Miss')\n",
        "test['Title'] = test['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# 직함 매핑\n",
        "test['Title'] = test['Title'].map(title_mapping)\n",
        "test['Title'] = test['Title'].fillna(0)\n",
        "\n",
        "# 객실 정보 처리\n",
        "test['HasCabin'] = test['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n",
        "\n",
        "# 범주형 변수 처리\n",
        "test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})\n",
        "test['Embarked'] = test['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
        "\n",
        "# 더미 변수 생성\n",
        "test_embarked_dummies = pd.get_dummies(test['Embarked'], prefix='Emb')\n",
        "test_pclass_dummies = pd.get_dummies(test['Pclass'], prefix='Pclass')\n",
        "\n",
        "# 최종 테스트 데이터프레임 생성\n",
        "X_test = pd.concat([\n",
        "    test[features],\n",
        "    test_embarked_dummies,\n",
        "    test_pclass_dummies\n",
        "], axis=1)\n",
        "\n",
        "# 스케일링 적용\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "\n",
        "# 예측 수행\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_predictions = (test_outputs > 0.5).float()\n",
        "\n",
        "# 제출 파일 생성\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test['PassengerId'],\n",
        "    'Survived': test_predictions.numpy().flatten().astype(int)\n",
        "})\n",
        "\n",
        "# CSV 파일로 저장\n",
        "submission.to_csv('submission03.csv', index=False)\n",
        "print(\"제출 파일 생성: submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC9vqOl0oviG",
        "outputId": "0b418401-6a69-4fd4-bf8d-322d1ed8128f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제출 파일 생성: submission.csv\n"
          ]
        }
      ]
    }
  ]
}